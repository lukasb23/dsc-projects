{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 News Group Classification\n",
    "\n",
    "Adapted from https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#assure that GPU is available to Keras \n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install spacy and spacy embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Text Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = \"data\"\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000 #messages are truncated to max. length of 1000\n",
    "MAX_NUM_WORDS = 20000      #top 20000 most occuring words in the text\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=5, micro=3, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    \n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        \n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            \n",
    "            if fname.isdigit():    \n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath, encoding='latin-1')\n",
    "                \n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                \n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndmn@kepler.unh.edu (...until kings become philosophers or philosophers become kings) writes:\\n>      Recently, RAs have been ordered (and none have resisted or cared about\\n> it apparently) to post a religious flyer entitled _The Soul Scroll: Thoughts\\n> on religion, spirituality, and matters of the soul_ on the inside of bathroom\\n> stall doors. (at my school, the University of New Hampshire) It is some sort\\n> of newsletter assembled by a Hall Director somewhere on campus. It poses a\\n> question about \\'spirituality\\' each issue, and solicits responses to be \\n> included in the next \\'issue.\\' It\\'s all pretty vague. I assume it\\'s put out\\n> by a Christian, but they\\'re very careful not to mention Jesus or the bible.\\n> I\\'ve heard someone defend it, saying \"Well it doesn\\'t support any one religion.\\n> \" So what??? This is a STATE university, and as a strong supporter of the\\n> separation of church and state, I was enraged.\\n> \\n>      What can I do about this?\\n\\nIt sounds to me like it\\'s just SCREAMING OUT for parody.  Give a copy to your\\nfriendly neighbourhood SubGenius preacher; with luck, he\\'ll run it through the\\nmental mincer and hand you back an outrageously offensive and gut-bustingly\\nfunny parody you can paste over the originals.\\n\\nI can see it now:\\n\\n                               The Stool Scroll\\n         Thoughts on Religion, Spirituality, and Matters of the Colon\\n\\n                       (You can use this text to wipe)\\n\\n\\nmathew\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "[19, 19, 19, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "print(labels[:5])\n",
    "print(labels[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528\n",
      "5116\n",
      "678\n",
      "219\n",
      "83\n",
      "814\n",
      "68\n",
      "226\n",
      "432\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "#Sequences contains tokens of words in messages \n",
    "for i in range(10): \n",
    "    print(len(sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_index stores our created word tokens indices \n",
    "type(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weds', 142473)\n",
      "('dfko', 108396)\n",
      "('pkit', 118941)\n",
      "('labatts', 74824)\n",
      "('subfile', 155732)\n"
     ]
    }
   ],
   "source": [
    "#the word_index dict contains the indices of the 174074 found words\n",
    "for i in range(5): \n",
    "    print(random.choice(list(word_index.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 1)\n",
      "('to', 2)\n",
      "('of', 3)\n",
      "('a', 4)\n",
      "('and', 5)\n",
      "('be', 16)\n"
     ]
    }
   ],
   "source": [
    "#ordered by frequency\n",
    "for word in ['the', 'to', 'of', 'a', 'and', 'be']: \n",
    "    print((word, word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 252472)\n",
      "('to', 127265)\n",
      "('of', 114002)\n",
      "('a', 109464)\n",
      "('and', 99868)\n",
      "('be', 32688)\n"
     ]
    }
   ],
   "source": [
    "#word counts can be accessed as well\n",
    "word_counts = tokenizer.word_counts\n",
    "for word in ['the', 'to', 'of', 'a', 'and', 'be']: \n",
    "    print((word, word_counts[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     6,    54,\n",
       "       10754,  1731,    26,  8090,   201,  1731,    26,  9652,  8455,\n",
       "          43,   107,   316,    52,     4,   346,   251,    17,  1341,\n",
       "          68,  4299,    39,     8,    58,   362,     9,    51,   200,\n",
       "          18,   575,  3020,     5,     9,     1,  2694,     3,    89,\n",
       "          25,   575,     2,   166,    24,    17,     6,     4,   648,\n",
       "          14,     8,     4,   108,   610,     2,    57,   105,    13,\n",
       "        6174,     4,   246,     1,   610,   654,    21,  1878,    89,\n",
       "        2518,  1304,     6,   316,    24,    17,     6,     1,   521,\n",
       "         120,    27,    70,     6,  1456,    22,  3020,   240,    27,\n",
       "          19,   817,     2,    16,   597,   262,     2,   180,     2,\n",
       "         883,     3,    89,    21,   135,     5,    17,    32,  1629,\n",
       "         251,   432,  1507,     9,    51,  1183,  3020,    38,  1032,\n",
       "         265,   166,   154,  3020,     5,    38,   252,   362,    12,\n",
       "           1,  2201,  2400,     7,    65,    84,    14,  1878,    23,\n",
       "          13,   179,     1,   134,  3231,    91,  3020,   182,    18,\n",
       "         575,   316,    24,   316,     8,    32,  1056,     6,   282,\n",
       "         238,   713,   599,     3,    93,   146,    61,    19,   107,\n",
       "         567,     1,  1259,     3, 18377,    46,    19,   599,   799,\n",
       "           3,  4089,   146,    17,     1,  1190,     3,     1,  3187,\n",
       "           1,  2106,  2288,    12,     1,    93,   329,     3,   429,\n",
       "        1456,     5,  3020,    88,    17,   559,    71,  9483,  5626,\n",
       "          11,   289,  1456,    52,   205,     1,   666,     3,   429,\n",
       "           5,  3020,     9,     3,  1456,     5,   255,     9,     3,\n",
       "         429,    21,    11,     8,  2860,     9,   316,  1183,     1,\n",
       "        2694,     3, 15003,   666,     1,   362,    12,     9,     8,\n",
       "          17,  6175,  7221,    28,  1351, 11222,     3,   316,    18,\n",
       "          83,  2270,   692,   142,     5,    61,    80,    19,    27,\n",
       "        8593,    80,   309,    19,    27,  1140,     7,   690,   216,\n",
       "           1,   414,    31,   501,    11,    25,     7,    84,     6,\n",
       "           1,  1185, 15004,    22,   729,     5,    11,    25,  1370,\n",
       "          21,   309,    21,  9190,  1015,    67,    27,    19,    29,\n",
       "          44,  1015,   121,    42,    27,  7403,   834,    15,     1,\n",
       "         329,    15,  1677,   316,   154,     1,   687,     3,     1,\n",
       "         342,     7,    65,   116,    53,   180,    11,     8,   615,\n",
       "          35,   802,   145,    27,    81,    50,   126,   501,   883,\n",
       "           3,     1,  1143,     5,    11,  1264,     9,    31,   258,\n",
       "         123,     3,    77,   151,  1950,  4089,    21,     1,  1019,\n",
       "           3,    71,  1295,     5,   114,     1,  2366,     3,  3020,\n",
       "           2,   543,    22,   429,     2,   870,    19,    17,  1202,\n",
       "        1687,   107,     4,  6764,  1231,     3,   870,    38,    16,\n",
       "         849,   262,   513,    11,   203,    32,   485,   861,    20,\n",
       "        1495,   344,     3,  5562,     5,  4194,    38,    16,   637,\n",
       "          48,   827,     9,  1025,     2,   130,   112,     7,  1859,\n",
       "         159,     5,     7,    65,    84,     4,    40,  1491,  2604,\n",
       "         432,     8,     9,   349,    23,  3020,     5,   429,     5,\n",
       "        1456,  2422,    71,  2309,  1043,    29,    91,     7,   157,\n",
       "         419,   163,     6,     1,   576,     3,   397, 10568,    29,\n",
       "         320,     2,  3189,     9,    58,    66, 15005,     6,   256,\n",
       "        1168,    40,  1202,   773,    50,   184,   185,    82,    14,\n",
       "           5,    23,   316,     8,     4,    91,   452,   290,    68,\n",
       "           2,    16,   145,     9,   316,    25,     4, 13456,     8,\n",
       "          17,  1202,  1687,     1,  1922,     5,  1031,   861,    20,\n",
       "           1,  4299,    19,   661,   151,    21,     1,   610,     9,\n",
       "         316,    25,     4, 13456, 12815,    15,     1,   476,     6,\n",
       "           1,  3189,     3,   316,   667,     8,    39,    55,    75,\n",
       "         362,    12,    11,    40,  1491,     5,    40,  2031,  2604,\n",
       "           8,   349,   114,     6,   138,   387,  2051,     9,     2,\n",
       "        1168,     3,   597,  1143,     6,   138,  1719,     6,   521,\n",
       "       13457,  6762,    17,   114,     2,   840,   174,     9,  4570,\n",
       "          19,    17,  1341,     1,   259,   835,    11,     8,    86,\n",
       "        1264,     9,   429,    52,    83,  5766,    80,   309,    19,\n",
       "           1, 10755, 11222,     2,    33,   883,    61,    34,    16,\n",
       "           1, 10755,     8,   346,   154,    55,     3,   100,  6680,\n",
       "           5,    11,     8,    17,   114,   945,     1,    76,   580,\n",
       "        5715,     8,   168,     3,     1,  6176,     5,    96,  1350,\n",
       "           4, 19951,  4472,     1, 10755,  1680,   252,     4,   858,\n",
       "         579,     3,     2,     1,   180,   142,     1, 17068,  1680,\n",
       "         391,     2,  1111,   174,     6,     1,   134,  1264,  1255,\n",
       "           8,     9,    31,    58,   180,   230,     3,     1,   666,\n",
       "          25,   750,     1,  1307,    29,  3083,   128,    15,     8,\n",
       "         457,   917,   538,     5,   253,     2,  3105,     4,     3,\n",
       "           1,   305,     3,  3020,     6,    75,   560,    40,    88,\n",
       "          17,    74,    35,     1,   485,     3,   429,   139,   221,\n",
       "          64,     5,  1403,   323,    15,   429,    19,   457,  2298,\n",
       "          24,    80,     8,     9,  2069,     2,     4,     3,   316,\n",
       "        5816], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Pad Sequences fills array up with 0s \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "#Convert Labels into 2d array\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label for text 10000\n",
    "labels[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices) #inplace!\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "X_test = data[-nb_validation_samples:]\n",
    "y_test = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15998, 1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  2726,  1791,    26],\n",
       "       [    0,     0,     0, ...,   410,  1673,  1821],\n",
       "       [    0,     0,     0, ...,    63,    63,    63],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    53, 17120,  2050],\n",
       "       [    0,     0,     0, ...,    17,  1147,   428],\n",
       "       [    0,     0,     0, ...,    69,   209,  7318]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.17 % of first train messages < 1000 tokens\n"
     ]
    }
   ],
   "source": [
    "#~97% of our training set samples < max_sequence_length of 1000  \n",
    "count = 0 \n",
    "for messages in X_train:\n",
    "    if messages[0] == 0: \n",
    "        count += 1 \n",
    "\n",
    "print(round(count / X_train.shape[0] * 100, 2), '% of first train messages < 1000 tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1070925 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in nlp.vocab: \n",
    "    \n",
    "    embeddings_index[word.text] = word.vector\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Polytheist', array([ 1.5644e-01, -2.9517e-01, -4.1215e-01, -9.8563e-02,  6.0967e-01,\n",
      "       -7.2976e-01,  4.7989e-01,  1.6789e-01,  2.4484e-01, -4.7923e-01,\n",
      "        7.7935e-01,  3.2632e-01,  4.7726e-02, -2.6023e-02,  7.6045e-01,\n",
      "       -1.1377e-01, -2.9051e-01, -1.7867e+00,  2.1130e-01,  6.5804e-01,\n",
      "       -5.0218e-01, -1.2106e-01, -6.0476e-01,  5.1975e-01,  1.2457e-01,\n",
      "        1.0774e-01,  3.7064e-01,  4.4262e-02,  1.8892e-01, -2.9232e-03,\n",
      "        1.7389e-01,  1.0232e-01,  2.4751e-01, -3.0006e-01, -5.5702e-03,\n",
      "       -1.1887e-01, -2.1895e-01, -6.3357e-01, -3.2615e-01, -2.0657e-01,\n",
      "        1.8058e-01,  2.0180e-01, -1.0955e-01, -2.2905e-01, -2.7891e-01,\n",
      "       -5.7444e-01,  5.7610e-01, -5.0193e-01, -2.6081e-01,  8.3600e-01,\n",
      "       -1.4143e-01,  2.4012e-01, -6.0781e-01, -3.4914e-01, -4.4395e-02,\n",
      "        2.5574e-01, -2.3381e-03, -1.4333e-01,  8.4756e-03,  4.5842e-01,\n",
      "       -1.1005e-03, -6.1238e-01, -1.8476e-01, -2.8646e-01, -4.4901e-01,\n",
      "       -3.6561e-02,  4.1014e-01, -2.0191e-01, -2.1450e-01, -7.9883e-02,\n",
      "       -1.8775e-01,  2.7266e-01, -4.7162e-01, -8.4099e-01, -3.7003e-01,\n",
      "        7.8634e-02, -5.1525e-01, -7.9494e-01,  1.0825e-01,  7.7978e-02,\n",
      "        3.9483e-01, -7.1783e-01,  7.6949e-01, -7.5964e-01,  5.8304e-02,\n",
      "        2.6854e-01,  6.1755e-01, -7.0301e-01, -2.6439e-01, -4.4632e-01,\n",
      "       -1.6998e-01, -1.0690e-01,  2.5695e-01,  6.9281e-01, -3.0671e-01,\n",
      "       -9.8120e-02,  5.7198e-03, -4.3637e-01,  4.1231e-01, -6.7251e-02,\n",
      "        3.3917e-01,  3.3182e-02, -5.4690e-01,  1.8618e-01, -2.5349e-01,\n",
      "       -1.8068e+00, -7.1515e-01, -3.6195e-01, -1.1573e-01, -2.6018e-01,\n",
      "        9.1952e-02,  6.2622e-01, -3.1879e-01, -1.4086e-01, -2.6132e-01,\n",
      "        7.9196e-01,  8.7612e-01, -2.2306e-02,  2.7996e-01, -1.7035e-01,\n",
      "        6.8729e-01, -6.5308e-01,  7.9904e-02,  5.5467e-01,  3.3994e-01,\n",
      "        4.9143e-02,  1.7262e-01,  1.0331e-02,  4.0032e-01, -3.2234e-01,\n",
      "       -1.0053e-01, -4.4446e-01, -7.0794e-01, -3.1038e-01, -5.0631e-01,\n",
      "       -3.1064e-01,  4.3683e-01, -2.3417e-01, -2.9445e-01, -2.0651e-01,\n",
      "        9.6453e-02,  4.7695e-01,  5.4486e-01, -2.4327e-01, -4.4931e-01,\n",
      "        3.0563e-03, -2.0149e-01, -4.3775e-01, -1.0891e+00, -2.7593e-01,\n",
      "        7.9862e-02,  2.2700e-01, -1.9980e-01,  9.0755e-02, -4.1651e-01,\n",
      "        2.9302e-01,  4.2789e-01, -5.4115e-01,  8.9032e-01,  1.5099e-01,\n",
      "       -1.9990e-01, -2.2697e-01,  1.8143e-02,  2.1373e-01,  6.5318e-01,\n",
      "        5.2902e-02, -5.5602e-01,  8.5981e-01,  1.2595e-02,  9.8919e-03,\n",
      "        6.4674e-01,  1.3123e-01,  9.0655e-01,  8.1090e-02, -2.2712e-01,\n",
      "        2.3006e-01,  4.7831e-01, -2.1224e-01, -7.0410e-01, -1.5559e-01,\n",
      "        6.4114e-01,  5.1310e-01, -1.3227e-01,  2.8139e-01, -5.0039e-01,\n",
      "        3.5427e-01,  3.0734e-01,  1.9579e-01,  6.3159e-01,  9.2974e-02,\n",
      "       -4.2542e-01,  2.5490e-01,  1.5769e-01,  1.4233e-01,  5.0926e-01,\n",
      "        2.7376e-02,  1.4924e-01,  1.0297e-01, -9.9156e-02,  1.4753e-01,\n",
      "        8.0119e-02,  4.8194e-01,  1.9132e-01, -8.6204e-01, -2.0179e-01,\n",
      "        1.7234e-01, -5.3419e-02,  4.1019e-01, -3.8530e-02, -4.9801e-01,\n",
      "        8.8928e-03,  3.0065e-01,  3.3725e-01, -6.8499e-02, -4.5079e-01,\n",
      "       -1.2180e+00, -3.0274e-01,  5.1774e-01,  1.8524e-01, -1.6335e-01,\n",
      "        1.5977e-01, -1.9051e-01,  3.1512e-01, -4.7688e-01, -3.6270e-02,\n",
      "        4.9898e-02, -2.2578e-01, -1.3583e-01, -8.7739e-01, -7.7590e-02,\n",
      "       -3.5025e-01, -2.0808e-01, -7.5860e-02,  2.5810e-01,  2.2510e-01,\n",
      "        2.5861e-02, -2.7802e-01, -2.8433e-01,  2.5912e-01, -7.5350e-02,\n",
      "       -4.5854e-01,  7.0670e-01,  2.4390e-01, -3.3206e-01,  3.7846e-01,\n",
      "       -4.5588e-02,  7.4460e-01, -4.6658e-01,  4.8433e-01,  3.2964e-01,\n",
      "       -7.6611e-02, -1.8129e-01, -2.2339e-01, -1.1795e-01,  3.6226e-01,\n",
      "       -3.4466e-01,  5.0876e-01, -3.2121e-01, -3.6867e-01,  3.7598e-01,\n",
      "       -2.7382e-01, -3.6193e-01, -2.9469e-02, -1.4462e-01,  3.4505e-01,\n",
      "        6.6147e-01, -1.5123e-01, -4.0462e-01, -2.0029e-01,  5.9086e-01,\n",
      "       -1.3640e-01, -4.8511e-01, -2.8608e-01, -2.3256e-01, -7.1801e-02,\n",
      "        7.7588e-01,  3.9503e-01,  1.3807e-01, -2.0320e-01, -1.0202e+00,\n",
      "        3.1421e-01, -8.9515e-01,  9.2955e-02, -1.2936e-01, -3.1057e-01,\n",
      "        7.7592e-02, -3.0894e-01, -2.9973e-01,  4.8016e-01,  7.6619e-01,\n",
      "       -3.0601e-01,  3.2403e-01, -6.7079e-01, -3.3819e-01, -3.8221e-01,\n",
      "        1.9349e-02, -1.8140e-01,  5.9673e-01, -1.3055e-01, -3.6121e-01],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(random.choice(list(embeddings_index.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remember our word_index**: <br>\n",
    "('the', 1)\n",
    "('to', 2)\n",
    "('of', 3)\n",
    "('a', 4)\n",
    "('and', 5)\n",
    "('be', 16)\n",
    "... <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill embedded matrix according to our word_index order\n",
    "# words not found in embedding index will be all-zeros.\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27204  , -0.06203  , -0.1884   ,  0.023225 , -0.018158 ,\n",
       "        0.0067192, -0.13877  ,  0.17708  ,  0.17709  ,  2.5882   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['the'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27204001, -0.06203   , -0.1884    ,  0.023225  , -0.018158  ,\n",
       "        0.0067192 , -0.13877   ,  0.17708001,  0.17709   ,  2.58820009])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1][:10] # --> 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2105"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#of the first 40000 words, we have no word vector for 8231 words \n",
    "len(np.where(~embedding_matrix[:20000].any(axis=1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 1000) dtype=int32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_1/embedding_lookup/Identity:0' shape=(?, 1000, 300) dtype=float32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/10\n",
      "15998/15998 [==============================] - 13s 817us/step - loss: 2.0248 - acc: 0.3175 - val_loss: 1.6196 - val_acc: 0.4414\n",
      "Epoch 2/10\n",
      "15998/15998 [==============================] - 10s 632us/step - loss: 1.0755 - acc: 0.6262 - val_loss: 0.9944 - val_acc: 0.6624\n",
      "Epoch 3/10\n",
      "15998/15998 [==============================] - 10s 632us/step - loss: 0.7887 - acc: 0.7322 - val_loss: 0.9026 - val_acc: 0.6979\n",
      "Epoch 4/10\n",
      "15998/15998 [==============================] - 10s 632us/step - loss: 0.6190 - acc: 0.7894 - val_loss: 0.8119 - val_acc: 0.7299\n",
      "Epoch 5/10\n",
      "15998/15998 [==============================] - 10s 633us/step - loss: 0.4802 - acc: 0.8405 - val_loss: 0.7387 - val_acc: 0.7707\n",
      "Epoch 6/10\n",
      "15998/15998 [==============================] - 10s 634us/step - loss: 0.3750 - acc: 0.8749 - val_loss: 0.8201 - val_acc: 0.7549\n",
      "Epoch 7/10\n",
      "15998/15998 [==============================] - 10s 636us/step - loss: 0.3034 - acc: 0.9010 - val_loss: 0.9170 - val_acc: 0.7329\n",
      "Epoch 8/10\n",
      "15998/15998 [==============================] - 10s 636us/step - loss: 0.2476 - acc: 0.9166 - val_loss: 0.7930 - val_acc: 0.7804\n",
      "Epoch 9/10\n",
      "15998/15998 [==============================] - 10s 635us/step - loss: 0.2130 - acc: 0.9305 - val_loss: 0.8838 - val_acc: 0.7852\n",
      "Epoch 10/10\n",
      "15998/15998 [==============================] - 10s 637us/step - loss: 0.1853 - acc: 0.9395 - val_loss: 0.8665 - val_acc: 0.7864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad48c416a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks \n",
    "\n",
    "- see Githb issue https://github.com/keras-team/keras/issues/9104 (poor accuracy)\n",
    "- Currenly only words as tokens, no windows or sequences --> possible extension "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
