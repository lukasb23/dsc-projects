{"nbformat_minor": 2, "cells": [{"source": "## Analysing User Clicks (Kelkoo.com)\n\nSource: https://archive.ics.uci.edu/ml/datasets/KASANDR", "cell_type": "markdown", "metadata": {}}, {"source": "Spark running on default HDInsight Cluster in Azure (2 head nodes, 4 workers); Data loaded into *'hdfs://{path}/lukasb23_dir/{file}'*.\n\nSetup:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-jupyter-spark-sql-use-portal\n<br> <br>\nAdjusting the executor memory necessary for model fitting:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\"executorMemory\": \"20G\"}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'pyspark3', u'executorMemory': u'20G'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "### Data Loading", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "#Define active namenode \npath = 'hn1-sparky.4r0ycwlvbjpuxisejhaosb1dyb.ax.internal.cloudapp.net'\n\n#Define csv file \nfile = 'train_de.csv'", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 250, "cell_type": "code", "source": "#Read data\ndf = spark.read.option(\"delimiter\", \"\\t\")\\\n          .option(\"header\", \"True\")\\\n          .option(\"inferSchema\", \"True\")\\\n          .csv('hdfs://{}/lukasb23_dir/{}'.format(path,file))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 27, "cell_type": "code", "source": "df.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+-----------+---------+--------------------+-------------------+------+\n|              userid|             offerid|countrycode| category|            merchant|            utcdate|rating|\n+--------------------+--------------------+-----------+---------+--------------------+-------------------+------+\n|fa937b779184527f1...|c5f63750c2b5b0166...|         de|100020213|f3c93baa0cf443084...|2016-06-14 17:28:47|     0|\n|f6c8958b9bc2d6033...|19754ec121b3a99ff...|         de|100020213|21a509189fb0875c3...|2016-06-14 17:28:48|     0|\n|02fe7ccf1de19a387...|5ac4398e4d8ad4167...|         de|   125801|b042951fdb45ddef8...|2016-06-14 17:28:50|     0|\n|9de5c06d0a16256b1...|be83df9772ec47fd2...|         de|   125801|4740b6c83b6e12e42...|2016-06-14 17:29:19|     0|\n|8d26ade603ea5473c...|3735290a415dc236b...|         de|   125801|8bf8f87492a799528...|2016-06-14 17:29:31|     0|\n+--------------------+--------------------+-----------+---------+--------------------+-------------------+------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 28, "cell_type": "code", "source": "df.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- userid: string (nullable = true)\n |-- offerid: string (nullable = true)\n |-- countrycode: string (nullable = true)\n |-- category: integer (nullable = true)\n |-- merchant: string (nullable = true)\n |-- utcdate: timestamp (nullable = true)\n |-- rating: integer (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 31, "cell_type": "code", "source": "df.head(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(userid='fa937b779184527f12e2d71c711e6411236d1ab59f8597d7494af26d194f0979', offerid='c5f63750c2b5b0166e55511ee878b7a3', countrycode='de', category=100020213, merchant='f3c93baa0cf4430849611cedb3a40ec4094d1d370be8417181da5d13ac99ef3d', utcdate=datetime.datetime(2016, 6, 14, 17, 28, 47), rating=0)]"}], "metadata": {"collapsed": false}}, {"execution_count": 38, "cell_type": "code", "source": "#Shape\nprint((df.count(), len(df.columns)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(15844717, 7)"}], "metadata": {"collapsed": false}}, {"execution_count": 153, "cell_type": "code", "source": "#Target variable\ntarget = df.select('rating').groupBy('rating').count()\ntarget.show()\n\ntotal_yes = target.head(2)[0][1] \ntotal_no = target.head(2)[1][1] \n\nprint('Target 1 zu 0: {}%'.format(round(total_yes/total_no,4)*100))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------+--------+\n|rating|   count|\n+------+--------+\n|     1|  705447|\n|     0|15139270|\n+------+--------+\n\nTarget 1 zu 0: 4.66%"}], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "df.select('countrycode').distinct().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+\n|countrycode|\n+-----------+\n|         de|\n+-----------+"}], "metadata": {"collapsed": false}}, {"source": "### Data Cleaning", "cell_type": "markdown", "metadata": {}}, {"execution_count": 44, "cell_type": "code", "source": "df.columns", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['userid', 'offerid', 'countrycode', 'category', 'merchant', 'utcdate', 'rating']"}], "metadata": {"collapsed": false}}, {"execution_count": 43, "cell_type": "code", "source": "#Check for missing values\nfor col in df.columns:    \n    print(col, ': ', df.filter((df[col] == \"\") | df[col].isNull()).count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "userid :  0\nofferid :  0\ncountrycode :  0\ncategory :  0\nmerchant :  0\nutcdate :  0\nrating :  0"}], "metadata": {"collapsed": false}}, {"source": "### Data Exploration", "cell_type": "markdown", "metadata": {}}, {"execution_count": 191, "cell_type": "code", "source": "from pyspark.sql.functions import *", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 53, "cell_type": "code", "source": "#Dates \nmin_date, max_date = df.select(min(\"utcdate\"), max(\"utcdate\")).first()\nmin_date, max_date", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(datetime.datetime(2016, 6, 1, 2, 0, 17), datetime.datetime(2016, 6, 14, 23, 52, 51))"}], "metadata": {"collapsed": false}}, {"execution_count": 66, "cell_type": "code", "source": "#Unique Values\nfor col in df.columns[:4]:    \n    print(col, ': ', df.select(col).distinct().count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "userid :  291485\nofferid :  2158859\ncategory :  271\nmerchant :  703"}], "metadata": {"collapsed": false}}, {"execution_count": 160, "cell_type": "code", "source": "df_yes = df.filter(df['rating'] == 1)\ndf_no = df.filter(df['rating'] == 0)\ndf_yes.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+---------+--------------------+-------------------+------+-------+----+\n|              userid|             offerid| category|            merchant|            utcdate|rating|weekday|hour|\n+--------------------+--------------------+---------+--------------------+-------------------+------+-------+----+\n|5bafdc0592dff7fa8...|b24526a20f4e4412d...|   138001|9f6c66333880924b1...|2016-06-02 17:50:48|     1|      5|  17|\n|6eb2fe43a01f9daf1...|2bfd670e616b8f088...|100354123|ab8863ef55e574c00...|2016-06-02 17:51:20|     1|      5|  17|\n|7c61831be00442150...|05aa287e0a53f6a5c...|100091613|418899436d9314d6a...|2016-06-02 17:53:50|     1|      5|  17|\n|7735de9a62b5bdd43...|9674edeb73f6e51ad...|   142101|c26503aa822d9652c...|2016-06-02 17:54:47|     1|      5|  17|\n|2014a976a7e6775f7...|9157b97ae21f55ed3...|   164401|b042951fdb45ddef8...|2016-06-02 17:55:45|     1|      5|  17|\n+--------------------+--------------------+---------+--------------------+-------------------+------+-------+----+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"source": "#### Categories", "cell_type": "markdown", "metadata": {}}, {"execution_count": 197, "cell_type": "code", "source": "#Categories Grouping \ncats = df.groupBy(\"category\").agg({'rating': 'mean', 'category': 'count'})\n\ncats = cats.withColumn('count(positive)', (col('avg(rating)') * col('count(category)')).cast('int'))\ncats = cats.sort(\"avg(rating)\", ascending=False)\ncats.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+------------------+---------------+---------------+\n| category|       avg(rating)|count(category)|count(positive)|\n+---------+------------------+---------------+---------------+\n|100020813|               1.0|             54|             54|\n|   121201|0.9902912621359223|            103|            102|\n|100345723|0.9705882352941176|            170|            165|\n|   128101|               0.9|             20|             18|\n|100333423|0.8773584905660378|            212|            186|\n+---------+------------------+---------------+---------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 213, "cell_type": "code", "source": "cats.filter(cats['count(positive)'] > 10000).show()\nprint('Total Yes: ', total_yes)\n\n#top 4 interesting, make up >30% of all clicks", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+--------------------+---------------+---------------+\n| category|         avg(rating)|count(category)|count(positive)|\n+---------+--------------------+---------------+---------------+\n|100010713|  0.5184821498016645|         102856|          53329|\n|   125801|  0.1227365557895503|         439706|          53968|\n|100020213| 0.08024719256855496|         663774|          53266|\n|100354123| 0.07432864699549051|         769044|          57162|\n|   130401|0.050088623711211526|         200285|          10032|\n|100434023|0.049709758354654014|         202073|          10045|\n|   134101| 0.04908641060584009|         226743|          11130|\n|100091613| 0.04691772227138838|         302828|          14208|\n|   142101| 0.04081416620078929|         302297|          12337|\n|   168001| 0.03791937619216083|         274187|          10397|\n|100232023| 0.03331888597215748|         366849|          12222|\n|   113501|0.028335950614609844|         461675|          13082|\n|   108301| 0.02050122659900939|         857656|          17583|\n|   108101|0.013961994014148183|         934537|          13048|\n+---------+--------------------+---------------+---------------+\n\nTotal Yes:  705447"}], "metadata": {"collapsed": false}}, {"source": "#### Merchants", "cell_type": "markdown", "metadata": {}}, {"execution_count": 204, "cell_type": "code", "source": "#Merchants Grouping \nmercs = df.groupBy('merchant').agg({'rating': 'mean', 'merchant': 'count'})\n\nmercs = mercs.withColumn('count(positive)', (col('avg(rating)') * col('count(merchant)')).cast('int'))\nmercs = mercs.sort(\"avg(rating)\", ascending=False)\nmercs.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+------------------+---------------+---------------+\n|            merchant|       avg(rating)|count(merchant)|count(positive)|\n+--------------------+------------------+---------------+---------------+\n|245d2c7b8e6fc4de4...|               1.0|             54|             54|\n|3e0c8ff0db6c0ba0a...|               1.0|             31|             31|\n|36e2130a3c07037b1...|0.9523809523809523|             21|             20|\n|ea0c486f7e2afef05...|0.9354838709677419|             31|             29|\n|cde1bb72b28d4a887...|0.9230769230769231|             13|             12|\n+--------------------+------------------+---------------+---------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 215, "cell_type": "code", "source": "mercs.filter(mercs['count(positive)'] > 10000).show()\nprint('Total Yes: ', total_yes)\n\n#top three >21% of clicks", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-------------------+---------------+---------------+\n|            merchant|        avg(rating)|count(merchant)|count(positive)|\n+--------------------+-------------------+---------------+---------------+\n|66863da8db7e6c51b...| 0.6265489999207334|          75694|          47425|\n|a7b2f269064dbe77e...|0.30314010305378086|         187669|          56890|\n|ac26975cf46eae989...| 0.1618780124357797|         314737|          50949|\n|70ea724342fb2d118...| 0.1512924051551128|          82714|          12514|\n|eb49b22a1bbd88fbd...|0.07760938578329883|         181125|          14057|\n|ab8863ef55e574c00...|0.06480676778336472|         579924|          37583|\n|5878d16d0c0691283...| 0.0536400383381256|         239970|          12872|\n|fca91704667a53350...|0.04780318920597915|         286529|          13697|\n|8497a9dd86ab3b7f1...|0.04425939471683428|         277523|          12283|\n|154f65f908a740682...| 0.0244791419290489|         484249|          11854|\n+--------------------+-------------------+---------------+---------------+\n\nTotal Yes:  705447"}], "metadata": {"collapsed": false}}, {"source": "**Does merchant no.1 sell cat no. 1?**", "cell_type": "markdown", "metadata": {}}, {"execution_count": 220, "cell_type": "code", "source": "merch_id = mercs.filter(mercs['count(positive)'] > 10000).head(1)[0][0]\n\ndf.filter(df['merchant'] == merch_id).groupby('category').count().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+-----+\n| category|count|\n+---------+-----+\n|100010713|75694|\n+---------+-----+"}], "metadata": {"collapsed": false}}, {"source": "Yes, solely. ", "cell_type": "markdown", "metadata": {}}, {"source": "**How about merchant No. 2?**", "cell_type": "markdown", "metadata": {}}, {"execution_count": 224, "cell_type": "code", "source": "merch_id = mercs.filter(mercs['count(positive)'] > 10000).head(2)[1][0]\n\ndf.filter(df['merchant'] == merch_id).groupby('category').count().sort('count', ascending=False).show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+-----+\n| category|count|\n+---------+-----+\n|100020213|68673|\n|   125801|64063|\n|   113501| 4770|\n|   142101| 4712|\n|   143101| 4562|\n|   133301| 3953|\n|   120901| 2963|\n|     6513| 2770|\n|100046613| 2180|\n|100367723| 2136|\n+---------+-----+\nonly showing top 10 rows"}], "metadata": {"collapsed": false}}, {"source": "Heavily invoilved in cats 2 & 3.", "cell_type": "markdown", "metadata": {}}, {"source": "#### Users", "cell_type": "markdown", "metadata": {}}, {"execution_count": 226, "cell_type": "code", "source": "#Users Grouping\nusers = df.groupBy('userid').agg({'rating': 'mean', 'userid': 'count'})\n\nusers = users.withColumn('count(positive)', (col('avg(rating)') * col('count(userid)')).cast('int'))\nusers = users.sort(\"avg(rating)\", ascending=False)\nusers.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----------+-------------+---------------+\n|              userid|avg(rating)|count(userid)|count(positive)|\n+--------------------+-----------+-------------+---------------+\n|b30b99b5627c4b0f6...|        1.0|            6|              6|\n|9e3dfcddda9bfd607...|        1.0|            5|              5|\n|bfc7f5a747eec2abd...|        1.0|            2|              2|\n|b112ac315de5042e0...|        1.0|            7|              7|\n|84c3e6783aee36656...|        1.0|            2|              2|\n+--------------------+-----------+-------------+---------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 235, "cell_type": "code", "source": "users.filter(users['count(positive)'] > 1000).show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-------------------+-------------+---------------+\n|              userid|        avg(rating)|count(userid)|count(positive)|\n+--------------------+-------------------+-------------+---------------+\n|6e2ab4134ce6b24d1...|0.30091743119266057|         4360|           1312|\n|7511572a7068fe6e7...|                0.3|         3550|           1065|\n|cad7e4a68117616ba...| 0.2992623814541623|         3796|           1136|\n|7625efac4a89c43c4...|0.29914529914529914|         6318|           1890|\n|4ba6cb76318d7db81...|0.29873150105708246|         4730|           1413|\n|cd5fc9305c30dfd50...|0.29870441458733205|         4168|           1245|\n|f0fbac7eb4c2c0ded...|0.29798870853916726|         5668|           1689|\n|314dc010def122b88...| 0.2978501045088086|         6698|           1995|\n|c08bfca37471d9c79...| 0.2968835429196282|         3658|           1086|\n|ce57395c3fe3f1037...|0.29661354581673305|         5020|           1489|\n+--------------------+-------------------+-------------+---------------+\nonly showing top 10 rows"}], "metadata": {"collapsed": false}}, {"source": "Got some power users here.", "cell_type": "markdown", "metadata": {}}, {"source": "#### Offers", "cell_type": "markdown", "metadata": {}}, {"execution_count": 246, "cell_type": "code", "source": "#Offers Grouping\noffers = df.groupBy('offerid').agg({'rating': 'mean', 'offerid': 'count'})\n\noffers = offers.withColumn('count(positive)', (col('avg(rating)') * col('count(offerid)')).cast('int'))\noffers = offers.sort(\"avg(rating)\", ascending=False)\noffers.show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----------+--------------+---------------+\n|             offerid|avg(rating)|count(offerid)|count(positive)|\n+--------------------+-----------+--------------+---------------+\n|3c0b61ef6ae1c918f...|        1.0|             1|              1|\n|67b9ac169bbd6506f...|        1.0|             4|              4|\n|9e6ed5473e4f27ded...|        1.0|             1|              1|\n|436394f91445351b2...|        1.0|             1|              1|\n|f02cb8e4667b90dfc...|        1.0|             2|              2|\n|9cb582f6f1d01fab8...|        1.0|             1|              1|\n|2e72138ae92fc046f...|        1.0|             1|              1|\n|30a07c874c2dac337...|        1.0|             1|              1|\n|990beb5468802a385...|        1.0|             4|              4|\n|4d9c77966a04b0b14...|        1.0|             2|              2|\n+--------------------+-----------+--------------+---------------+\nonly showing top 10 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 248, "cell_type": "code", "source": "offers.filter(offers['count(positive)'] > 100).show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+------------------+--------------+---------------+\n|             offerid|       avg(rating)|count(offerid)|count(positive)|\n+--------------------+------------------+--------------+---------------+\n|a066572754a00f7a0...|0.9955156950672646|           223|            222|\n|ae19aab375ac925bd...|0.9913793103448276|           232|            230|\n|61ba4686f0a6b704a...|0.9911764705882353|           340|            337|\n|c07db2553dc287e0d...|0.9902912621359223|           103|            102|\n|e27570f3348a9fec4...|0.9883551673944687|           687|            679|\n|5c2f1d277d6922bb7...|0.9870689655172413|           232|            229|\n|ea268a1ab5ba20bdd...| 0.981203007518797|           266|            261|\n|a1cb8243ccdbfda93...|0.9809523809523809|           105|            103|\n|01c0d039ff17aa7f4...|0.9752066115702479|           121|            118|\n|30a81c7e560b76f51...|0.9635036496350365|           137|            132|\n+--------------------+------------------+--------------+---------------+\nonly showing top 10 rows"}], "metadata": {"collapsed": false}}, {"source": "### A tiny little bit of feature engineering ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 255, "cell_type": "code", "source": "from pyspark.sql.functions import hour, dayofweek\nfrom pyspark.ml import Pipeline, Transformer\nfrom pyspark.sql import DataFrame\n\ndf = df.withColumn('weekday', dayofweek('utcdate'))\\\n       .withColumn('hour', hour('utcdate'))\\\n       .drop(\"countrycode\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+---------+--------------------+-------------------+------+-------+----+\n|              userid|             offerid| category|            merchant|            utcdate|rating|weekday|hour|\n+--------------------+--------------------+---------+--------------------+-------------------+------+-------+----+\n|fa937b779184527f1...|c5f63750c2b5b0166...|100020213|f3c93baa0cf443084...|2016-06-14 17:28:47|     0|      3|  17|\n|f6c8958b9bc2d6033...|19754ec121b3a99ff...|100020213|21a509189fb0875c3...|2016-06-14 17:28:48|     0|      3|  17|\n|02fe7ccf1de19a387...|5ac4398e4d8ad4167...|   125801|b042951fdb45ddef8...|2016-06-14 17:28:50|     0|      3|  17|\n|9de5c06d0a16256b1...|be83df9772ec47fd2...|   125801|4740b6c83b6e12e42...|2016-06-14 17:29:19|     0|      3|  17|\n|8d26ade603ea5473c...|3735290a415dc236b...|   125801|8bf8f87492a799528...|2016-06-14 17:29:31|     0|      3|  17|\n+--------------------+--------------------+---------+--------------------+-------------------+------+-------+----+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 262, "cell_type": "code", "source": "#Weekdays Grouping \nweekdays = df.groupBy('weekday').agg({'rating': 'mean', 'weekday': 'count'})\n\nweekdays = weekdays.withColumn('count(positive)', (col('avg(rating)') * col('count(weekday)')).cast('int'))\nweekdays = weekdays.sort(\"avg(rating)\", ascending=False)\nweekdays.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------------------+--------------+---------------+\n|weekday|         avg(rating)|count(weekday)|count(positive)|\n+-------+--------------------+--------------+---------------+\n|      3| 0.10241874599458782|       2103404|         215428|\n|      4|0.041626950360412696|       1955536|          81403|\n|      2|0.039021738813080464|       2055034|          80191|\n|      6| 0.03882920266295414|       1923578|          74691|\n|      5| 0.03401875987435719|       2403321|          81758|\n|      1| 0.03237889335234592|       2991702|          96868|\n|      7|0.031137470347931424|       2412142|          75108|\n+-------+--------------------+--------------+---------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 211, "cell_type": "code", "source": "#Hour Grouping \nhours = df.groupBy('hour').agg({'rating': 'mean', 'hour': 'count'})\n\nhours = hours.withColumn('count(positive)', (col('avg(rating)') * col('count(hour)')).cast('int'))\nhours = hours.sort(\"avg(rating)\", ascending=False)\nhours.show(24)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----+-----------+--------------------+---------------+\n|hour|count(hour)|         avg(rating)|count(positive)|\n+----+-----------+--------------------+---------------+\n|   0|      27285|  0.3720725673446949|          10152|\n|  20|     124057|  0.3546514908469494|          43997|\n|   1|      19329| 0.32019245693000153|           6189|\n|  21|     140220| 0.31691627442590214|          44438|\n|  22|     128301| 0.30285812269584805|          38857|\n|  23|     139157|  0.2634003319991089|          36654|\n|  19|     264503| 0.16856519585789198|          44586|\n|  18|     581059| 0.08287970756842249|          48158|\n|  17|    1275499| 0.04213723413346463|          53746|\n|   7|     499146| 0.03726164288604937|          18599|\n|  16|    1558179| 0.03268302293895631|          50926|\n|  10|    1348360| 0.03207600344121748|          43250|\n|   8|     771206|0.031104529788409323|          23988|\n|   9|    1022504|0.029534358789794466|          30199|\n|  11|    1330400| 0.02866882140709561|          38141|\n|   4|     128428|0.027307129286448437|           3507|\n|  15|    1498873|0.026956253131519482|          40404|\n|  12|    1356433|  0.0266787965199903|          36188|\n|  13|    1373594|0.026670180562815504|          36634|\n|   5|     152877|0.026537674077853436|           4056|\n|   3|     161215| 0.02549390565394039|           4110|\n|  14|    1423326| 0.02542987340918384|          36195|\n|   6|     308217|0.025417157392356685|           7834|\n|   2|     212549|0.021820850721480694|           4638|\n+----+-----------+--------------------+---------------+"}], "metadata": {"collapsed": false}}, {"source": "### Model Buidling from Start", "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "from pyspark.ml import Pipeline, Transformer\nfrom pyspark.mllib.util import MLUtils\nfrom pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import hour, dayofweek", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "#Define active namenode \npath = 'hn1-sparky.4r0ycwlvbjpuxisejhaosb1dyb.ax.internal.cloudapp.net'\n\n#Define csv file \nfile = 'train_de.csv'", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "#Read data raw\ndf = spark.read.option(\"delimiter\", \"\\t\")\\\n          .option(\"header\", \"True\")\\\n          .option(\"inferSchema\", \"True\")\\\n          .csv('hdfs://{}/lukasb23_dir/{}'.format(path,file))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 83, "cell_type": "code", "source": "# Custom Transformer\nclass CustomTransformer(Transformer):\n\n    def __init__(self):\n        super(CustomTransformer, self).__init__()\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        \n        df = df.withColumn('weekday', dayofweek('utcdate'))\\\n               .withColumn('hour', hour('utcdate'))\\\n               .drop(\"countrycode\")\n        return df\n    \n#String Indexer \nindexer_cat = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n#indexer_user = StringIndexer(inputCol=\"userid\", outputCol=\"userid_index\") \nindexer_merc = StringIndexer(inputCol=\"merchant\", outputCol=\"merchant_index\")\n\n#One Hot Encoder\nvectors = [\"category_vec\", \"merchant_vec\"]\nencoder = OneHotEncoderEstimator(\n    inputCols=[\"category_index\", \"merchant_index\"],  \n    outputCols=vectors\n)\n\n#Feature Assembler\nvectorizer = VectorAssembler(inputCols=vectors+['weekday', 'hour'], outputCol='features')\n\n#Scaler\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=True, withMean=False)\n\n#Modelling \nlr = LogisticRegression(featuresCol = 'scaledFeatures', labelCol = 'rating', maxIter=10)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Feature Userid would result in +291.000 columns on 15m features; kicked for reasons of feasibility.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 87, "cell_type": "code", "source": "#Modelling via Pipeline\npipeline = Pipeline(stages=[indexer_cat, indexer_merc, encoder, \n                         CustomTransformer(), vectorizer, scaler, lr])\n\nmodel = pipeline.fit(df)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Training time: approx. 30secs. <br>\nYARN Memory > 50% used.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 88, "cell_type": "code", "source": "trainingSummary = model.stages[-1].summary\n\ntrainingSummary.roc.show()\nprint(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-------------------+\n|                 FPR|                TPR|\n+--------------------+-------------------+\n|                 0.0|                0.0|\n|0.002153736606850925|0.14141246613849093|\n|0.006552825862805...| 0.1973684770081948|\n|0.009952065059940143|0.23335558872601345|\n| 0.01351405979284338| 0.2465160387669095|\n| 0.01695947030471086|0.25820649885817076|\n| 0.02018327171653587|0.27248822377868215|\n| 0.02094559381000537|0.27578825907545146|\n|0.023791107497257134|0.29007140153689787|\n| 0.02712151906928141|0.30493006561796987|\n|  0.0308175361163385|  0.318758177439269|\n| 0.03403057082673075|0.33181939961471235|\n| 0.03839762419191942|0.34629674518425907|\n|0.043001941308927046|0.35940191112868863|\n| 0.04885473341845412| 0.3722462495410711|\n|0.054973786714947286| 0.3847815640296153|\n| 0.05512438842823993|0.38511752123121934|\n| 0.06098907014671117| 0.3976216498191927|\n| 0.06752194788784399|0.40964523203018793|\n| 0.07589441234616993| 0.4224626371648047|\n+--------------------+-------------------+\nonly showing top 20 rows\n\nareaUnderROC: 0.7398957989975802"}], "metadata": {"collapsed": false}}, {"source": "#### Test Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 94, "cell_type": "code", "source": "#Read data raw\ndf_test = spark.read.option(\"delimiter\", \"\\t\")\\\n          .option(\"header\", \"True\")\\\n          .option(\"inferSchema\", \"True\")\\\n          .csv('hdfs://{}/lukasb23_dir/test_de.csv'.format(path,file))", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 95, "cell_type": "code", "source": "predictions = model.transform(df_test)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 99, "cell_type": "code", "source": "predictions", "outputs": [{"output_type": "stream", "name": "stdout", "text": "DataFrame[userid: string, offerid: string, category: int, merchant: string, utcdate: timestamp, rating: int, category_index: double, merchant_index: double, category_vec: vector, merchant_vec: vector, weekday: int, hour: int, features: vector, scaledFeatures: vector, rawPrediction: vector, probability: vector, prediction: double]"}], "metadata": {"collapsed": false}}, {"source": "Example like in https://spark.apache.org/docs/2.3.0/ml-pipeline.html. \nHowever, access to predictions is error-prone...", "cell_type": "markdown", "metadata": {}}, {"execution_count": 100, "cell_type": "code", "source": "predictions.show(1)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling o5386.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 326.0 failed 4 times, most recent failure: Lost task 0.3 in stage 326.0 (TID 2858, wn5-sparky.4r0ycwlvbjpuxisejhaosb1dyb.ax.internal.cloudapp.net, executor 3): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: f98b17e16bff576aa2eba0d7ff29368465f774fe8fd305226a3f8dc7f19bb0b6.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: f98b17e16bff576aa2eba0d7ff29368465f774fe8fd305226a3f8dc7f19bb0b6.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\", line 350, in show\n    print(self._jdf.showString(n, 20, vertical))\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o5386.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 326.0 failed 4 times, most recent failure: Lost task 0.3 in stage 326.0 (TID 2858, wn5-sparky.4r0ycwlvbjpuxisejhaosb1dyb.ax.internal.cloudapp.net, executor 3): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: f98b17e16bff576aa2eba0d7ff29368465f774fe8fd305226a3f8dc7f19bb0b6.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: f98b17e16bff576aa2eba0d7ff29368465f774fe8fd305226a3f8dc7f19bb0b6.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n\n\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}